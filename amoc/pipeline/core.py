import logging
import os
import re
from typing import List, Tuple, Optional, Iterable
import pandas as pd
from spacy.tokens import Span, Token
import networkx as nx

from amoc.graph import Graph, Node, Edge, NodeType, NodeSource
from amoc.viz.graph_plots import plot_amoc_triplets
from amoc.llm.vllm_client import VLLMClient
from amoc.nlp.spacy_utils import (
    get_concept_lemmas,
    canonicalize_node_text,
    get_content_words_from_sent,
)
from collections import deque
from amoc.config.paths import OUTPUT_ANALYSIS_DIR


def _sanitize_filename_component(component: str, max_len: int = 80) -> str:
    component = (component or "").replace("\n", " ").strip()
    component = component[:max_len]
    component = re.sub(r"[\\/:*?\"<>|]", "_", component)
    component = re.sub(r"\s+", "_", component)
    return component or "unknown"


class AMoCv4:
    GENERIC_RELATION_LABELS = {
        "has_property",
        "is_type_of",
        "is_part_of",
        "related_to",
        "has_attribute",
        "is_a",
        "appears",
        "contains",
        "includes",
        "include",
        "contain",
        "part_of",
        "associated_with",
        "is associated with",
        "|eot id|",
        "refers to",
        "is an",
        "is mentioned in",
    }

    ENFORCE_ATTACHMENT_CONSTRAINT = False
    ACTIVATION_MAX_DISTANCE = 2
    RELATION_BLACKLIST = {"describes", "is_at_stake"}

    # Labels acceptable ONLY for hub-anchoring edges (relaxed validation)
    HUB_EDGE_ACCEPTABLE_LABELS = {
        "is",
        "has",
        "describes",
        "relates_to",
        "related_to",
        "is_related_to",
        "characterizes",
    }

    def __init__(
        self,
        persona_description: str,
        story_text: str,
        vllm_client: VLLMClient,
        max_distance_from_active_nodes: int,
        max_new_concepts: int,
        max_new_properties: int,
        context_length: int,
        edge_forget: int,
        nr_relevant_edges: int,
        spacy_nlp,
        debug: bool = False,
        persona_age: Optional[int] = None,
        strict_reactivate_function: bool = True,
        strict_attachament_constraint: bool = True,
        single_anchor_hub: bool = True,
        matrix_dir_base: Optional[str] = None,
    ) -> None:
        self.persona = persona_description
        self.story_text = story_text
        self.matrix_dir_base = matrix_dir_base or OUTPUT_ANALYSIS_DIR
        self.client = vllm_client
        self.model_name = vllm_client.model_name
        self.persona_age = persona_age

        if not isinstance(story_text, str) or not story_text.strip():
            raise ValueError("story_text must be a non-empty string")

        self.max_distance_from_active_nodes = max_distance_from_active_nodes
        self.max_new_concepts = max_new_concepts
        self.max_new_properties = max_new_properties
        self.context_length = context_length
        self.edge_forget = edge_forget
        self.nr_relevant_edges = nr_relevant_edges

        self.graph = Graph()
        self.spacy_nlp = spacy_nlp

        if self.spacy_nlp is None:
            raise RuntimeError("AMoCv4 requires a spaCy nlp object (spacy_nlp).")

        self.debug = debug
        # Cache story lemmas for quick membership checks (currently unused)
        story_doc = self.spacy_nlp(story_text)
        self.story_lemmas = {tok.lemma_.lower() for tok in story_doc if tok.is_alpha}
        self._prev_active_nodes_for_plot: set[Node] = set()
        self._cumulative_deactivated_nodes_for_plot: set[Node] = set()
        self._viz_positions: dict[str, tuple[float, float]] = {}
        self._recently_deactivated_nodes_for_inference: set[Node] = set()
        self._anchor_nodes: set[Node] = set()
        self._explicit_nodes_current_sentence: set[Node] = set()
        self.strict_reactivate_function = strict_reactivate_function
        self.strict_attachament_constraint = strict_attachament_constraint
        self.single_anchor_hub = single_anchor_hub
        self._current_sentence_text: str = ""
        # Separate memory (cumulative) vs salience (active) graphs for auditing.
        self.cumulative_graph = nx.MultiDiGraph()
        self.active_graph = nx.MultiDiGraph()
        # Stable triplet introduction index: (subj, rel, obj) -> introduced_at_sentence
        self._triplet_intro: dict[tuple[str, str, str], int] = {}
        # Append-only cumulative records (one row per active episode)
        self._cumulative_triplet_records: list[dict] = []
        self._fixed_hub = None
        # Track current hub for visualization (blue node) per Invariant 3.0
        self._current_hub: Optional[Node] = None
        # Store hub-anchored edge explanations for current sentence (for plot subtitle)
        self._hub_edge_explanations: List[str] = []
        # Track connectivity failures for explicit diagnostics per Invariant 3.4
        self._connectivity_failures: List[Tuple[str, str, str]] = []

    def _node_token_for_matrix(self, node: Node) -> str:
        return (node.get_text_representer() or "").strip().lower()

    def _distances_from_sources_active_edges(
        self, sources: set[Node], max_distance: int
    ) -> dict[Node, int]:
        if not sources:
            return {}
        distances: dict[Node, int] = {s: 0 for s in sources}
        queue: deque[Node] = deque(sources)
        while queue:
            node = queue.popleft()
            dist = distances[node]
            if dist >= max_distance:
                continue
            for edge in node.edges:
                if not edge.active:
                    continue
                neighbor = (
                    edge.dest_node if edge.source_node == node else edge.source_node
                )
                if neighbor in distances:
                    continue
                distances[neighbor] = dist + 1
                queue.append(neighbor)
        return distances

    def _record_sentence_activation(
        self,
        sentence_id: int,
        explicit_nodes: List[Node],
        newly_inferred_nodes: set[Node],
    ) -> None:
        def _to_landscape_score(raw_score: float) -> float:
            # Transform AMoC "distance" style (0=most active) into Landscape style (5=most active).
            val = 5.0 - float(raw_score)
            if val < 0.0:
                return 0.0
            if val > 5.0:
                return 5.0
            return val

        explicit_set = set(explicit_nodes)
        distances = self._distances_from_sources_active_edges(
            explicit_set, max_distance=self.ACTIVATION_MAX_DISTANCE
        )

        token_to_raw_score: dict[str, int] = {}
        node_raw_score: dict[Node, int] = {}

        # Step 2: explicit nodes reset to 0 (non-negotiable)
        for node in explicit_set:
            token = self._node_token_for_matrix(node)
            if token:
                token_to_raw_score[token] = 0
                node_raw_score[node] = 0

        # Step 5: newly inferred nodes start at 1 (never 0)
        for node in newly_inferred_nodes:
            if node in explicit_set:
                continue
            token = self._node_token_for_matrix(node)
            if token:
                token_to_raw_score[token] = 1
                node_raw_score[node] = 1

        # Step 3/4: carried-over nodes within range, score = distance
        for node, dist in distances.items():
            if node in explicit_set:
                continue
            if dist <= 0:
                continue
            token = self._node_token_for_matrix(node)
            if not token:
                continue
            if token in token_to_raw_score:
                continue
            token_to_raw_score[token] = dist
            node_raw_score[node] = dist

        # Convert node scores to Landscape scale and record.
        for token, raw_score in token_to_raw_score.items():
            self._amoc_matrix_records.append(
                {
                    "sentence": sentence_id,
                    "token": token,
                    "score": _to_landscape_score(raw_score),
                }
            )

        # Add verb (edge-label) activations: take the max activation of connected nodes minus 0.5.
        verb_scores: dict[str, float] = {}
        for edge in self.graph.edges:
            if not edge.active:
                continue
            label = (edge.label or "").strip().lower()
            if not label:
                continue
            src_tok = self._node_token_for_matrix(edge.source_node)
            dst_tok = self._node_token_for_matrix(edge.dest_node)
            if not src_tok or not dst_tok:
                continue
            src_raw = node_raw_score.get(edge.source_node, edge.source_node.score)
            dst_raw = node_raw_score.get(edge.dest_node, edge.dest_node.score)
            src_act = _to_landscape_score(src_raw)
            dst_act = _to_landscape_score(dst_raw)
            verb_act = max(src_act, dst_act) - 0.5
            if verb_act < 0.0:
                verb_act = 0.0
            prev = verb_scores.get(label)
            if prev is None or verb_act > prev:
                verb_scores[label] = verb_act

        for token, score in verb_scores.items():
            self._amoc_matrix_records.append(
                {"sentence": sentence_id, "token": token, "score": score}
            )

    def _infer_edges_to_recently_deactivated(
        self,
        current_sentence_nodes: List[Node],
        current_sentence_words: List[str],
        current_text: str,
    ) -> List[Edge]:
        if not self.ENFORCE_ATTACHMENT_CONSTRAINT:
            return []
        recent = [
            n
            for n in self._recently_deactivated_nodes_for_inference
            if n in self.graph.nodes
        ]
        if not recent or not current_sentence_nodes:
            return []

        candidate_pairs: set[frozenset[Node]] = set()
        for node in current_sentence_nodes:
            for other in recent:
                if node == other:
                    continue
                if self._has_edge_between(node, other):
                    continue
                candidate_pairs.add(frozenset((node, other)))
        if not candidate_pairs:
            return []

        nodes_for_prompt = {n for pair in candidate_pairs for n in pair}

        def _node_line(node: Node) -> str:
            return f" - ({node.get_text_representer()}, {node.node_type})\n"

        nodes_from_text = "".join(
            _node_line(n)
            for n in sorted(nodes_for_prompt, key=lambda x: x.get_text_representer())
        )
        graph_nodes_repr = self.graph.get_nodes_str(list(nodes_for_prompt))
        graph_edges_repr, _ = self.graph.get_edges_str(
            list(nodes_for_prompt), only_text_based=False
        )

        try:
            new_relationships = self.client.get_new_relationships(
                nodes_from_text,
                graph_nodes_repr,
                graph_edges_repr,
                current_text,
                self.persona,
            )
        except Exception:
            logging.error("Targeted LLM edge inference failed", exc_info=True)
            return []

        added: List[Edge] = []
        for idx, relationship in enumerate(new_relationships):
            if relationship is None or isinstance(relationship, (int, float, bool)):
                continue
            if isinstance(relationship, dict):
                subj = relationship.get("subject") or relationship.get("head")
                rel = relationship.get("relation") or relationship.get("predicate")
                obj = relationship.get("object") or relationship.get("tail")
                if not (subj and rel and obj):
                    continue
                relationship = (str(subj), str(rel), str(obj))
            if not isinstance(relationship, (list, tuple)) or len(relationship) != 3:
                continue

            subj, rel, obj = relationship
            subj = self._normalize_endpoint_text(subj, is_subject=True) or None
            obj = self._normalize_endpoint_text(obj, is_subject=False) or None
            if subj is None or obj is None:
                continue
            if not subj or not obj:
                continue
            if not isinstance(subj, str) or not isinstance(obj, str):
                continue
            edge_label = rel.replace("(edge)", "").strip()
            if not self._is_valid_relation_label(edge_label):
                continue

            subj_node = self._find_node_by_text(subj, nodes_for_prompt)
            obj_node = self._find_node_by_text(obj, nodes_for_prompt)
            if subj_node is None or obj_node is None:
                continue
            pair_key = frozenset((subj_node, obj_node))
            if pair_key not in candidate_pairs:
                continue
            if self._has_edge_between(subj_node, obj_node):
                continue

            edge = self._add_edge(subj_node, obj_node, edge_label, self.edge_forget)
            if edge:
                added.append(edge)
        return added

    def _passes_attachment_constraint(
        self,
        subject: str,
        obj: str,
        current_sentence_words: List[str],
        current_sentence_nodes: List[Node],
        graph_active_nodes: List[Node],
        graph_active_edge_nodes: Optional[set[Node]] = None,
    ) -> bool:
        # Connectivity constraint: avoid introducing edges that would form a new
        # disconnected component. Accept an edge only if it (a) touches the current
        # sentence and (b) attaches to a node already in the active graph (or, if
        # no edges are active, any existing node in the graph). This guard is always
        # enforced to keep the graph as a single component, regardless of the flag.

        active_edge_nodes = graph_active_edge_nodes or set()
        active_nodes_pool: set[Node] = set(graph_active_nodes) | active_edge_nodes
        anchor_nodes = self._anchor_nodes

        # If the graph is empty, allow seeding.
        if not self.graph.nodes:
            return True

        # If no active edges exist, fall back to the current anchor to keep
        # connectivity. If the anchor is empty (pre-seed), we allow the first edge.
        if not active_edge_nodes:
            active_nodes_pool |= anchor_nodes

        subject = canonicalize_node_text(self.spacy_nlp, subject)
        obj = canonicalize_node_text(self.spacy_nlp, obj)

        def _lemma_key(text: str) -> tuple[str, ...]:
            return tuple(get_concept_lemmas(self.spacy_nlp, text))

        sentence_lemma_keys = {tuple(n.lemmas) for n in current_sentence_nodes}
        active_lemma_keys = {tuple(n.lemmas) for n in active_nodes_pool}
        anchor_lemma_keys = {tuple(n.lemmas) for n in anchor_nodes}

        subj_key = _lemma_key(subject)
        obj_key = _lemma_key(obj)

        touches_sentence = (
            subject in current_sentence_words
            or obj in current_sentence_words
            or subj_key in sentence_lemma_keys
            or obj_key in sentence_lemma_keys
        )
        touches_active = subj_key in active_lemma_keys or obj_key in active_lemma_keys
        attaches_anchor = (
            not anchor_lemma_keys
            or subj_key in anchor_lemma_keys
            or obj_key in anchor_lemma_keys
        )
        memory_lemma_keys = {tuple(n.lemmas) for n in self.graph.nodes}
        touches_memory = subj_key in memory_lemma_keys or obj_key in memory_lemma_keys

        # Enforce connectivity: at least one endpoint must touch the existing graph.
        # touches_sentence alone is not sufficient as it doesn't guarantee connectivity.
        if not self.strict_attachament_constraint:
            return touches_memory

        # In strict mode, require connection to active nodes or existing memory.
        # This ensures the graph remains connected.
        return touches_active or touches_memory

    def _add_edge(
        self,
        source_node: Node,
        dest_node: Node,
        label: str,
        edge_forget: int,
        created_at_sentence: Optional[int] = None,
    ) -> Optional[Edge]:
        """
        Add an edge to the graph following the original AMoC v4 paper approach.

        Per Invariant 3.2: Edges may be verbs or adjectives - no strict POS validation.
        Per Invariant 3.4: Only empty/unlabeled edges are forbidden.

        The original AMoC v4 paper (old_code.py lines 134-141) adds edges with
        minimal validation. Strict POS-based validation was causing over-rejection
        of valid LLM labels.
        """
        # Per Invariant 3.4: Reject only empty/missing labels
        if not label or not isinstance(label, str) or not label.strip():
            logging.debug(
                "Rejected edge with empty/missing label: %s -[%s]-> %s",
                source_node.get_text_representer() if source_node else "None",
                label,
                dest_node.get_text_representer() if dest_node else "None",
            )
            return None

        # Optional attachment constraint for maintaining connectivity
        if self.strict_attachament_constraint and self.graph.edges:
            G = nx.Graph()
            for e in self.graph.edges:
                G.add_edge(e.source_node, e.dest_node)

            # Attachable: existing graph nodes + anchors + current explicit nodes
            existing_nodes = set(G.nodes())
            attachable = (
                existing_nodes
                | self._anchor_nodes
                | getattr(self, "_explicit_nodes_current_sentence", set())
            )

            # Only reject if BOTH endpoints are completely disconnected
            if source_node not in attachable and dest_node not in attachable:
                logging.debug(
                    "Rejected disconnected edge: %s -[%s]-> %s",
                    source_node.get_text_representer(),
                    label,
                    dest_node.get_text_representer(),
                )
                return None

        use_sentence = (
            created_at_sentence
            if created_at_sentence is not None
            else getattr(self, "_current_sentence_index", None)
        )

        edge = self.graph.add_edge(
            source_node,
            dest_node,
            label,
            edge_forget,
            created_at_sentence=use_sentence,
        )

        if edge:
            trip_id = (
                edge.source_node.get_text_representer(),
                edge.label,
                edge.dest_node.get_text_representer(),
            )
            if trip_id not in self._triplet_intro:
                self._triplet_intro[trip_id] = (
                    use_sentence if use_sentence is not None else -1
                )

            self._record_edge_in_graphs(edge, self._current_sentence_index)

        return edge

    def reset_graph(self) -> None:
        self.graph = Graph()
        self._anchor_nodes = set()

    def _enforce_graph_connectivity(self) -> None:
        # No-op: do not prune edges after addition; connectivity is enforced at add time.
        return

    # =========================================================================
    # REDESIGNED STRICT ATTACHMENT CONSTRAINT - TWO-STEP CONSTRUCTION
    # Aligned with AMoC v4 Paper Section 3.1.2
    # =========================================================================

    def _is_explicit_backbone_connected(self, explicit_nodes: List[Node]) -> bool:
        if len(explicit_nodes) <= 1:
            return True
        G = nx.Graph()
        explicit_set = set(explicit_nodes)
        for node in explicit_nodes:
            G.add_node(node)
        for edge in self.graph.edges:
            if edge.source_node in explicit_set and edge.dest_node in explicit_set:
                G.add_edge(edge.source_node, edge.dest_node)
        return nx.is_connected(G)

    def _select_hub_node(self, explicit_nodes: List[Node]) -> Optional[Node]:
        """
        Select the hub node per Invariant 3.0.

        INVARIANT 3.0 (Hub Node Invariant - Foundational, Non-Negotiable):
        - Every per-sentence graph always has at least one hub node
        - The hub node is explicitly designated
        - The hub node is visually represented as a blue node
        - There is no valid per-sentence graph without a hub node

        Returns None only if explicit_nodes is empty (no graph to build).
        """
        if not explicit_nodes:
            logging.warning(
                "INVARIANT 3.0: No explicit nodes provided - cannot select hub"
            )
            return None

        # Use fixed hub if already set and still in explicit nodes
        if self._fixed_hub is not None and self._fixed_hub in explicit_nodes:
            self._current_hub = self._fixed_hub
            return self._fixed_hub

        # Prefer CONCEPT nodes as hub candidates
        concepts = [n for n in explicit_nodes if n.node_type == NodeType.CONCEPT]
        candidates = concepts if concepts else explicit_nodes

        def node_degree(n: Node) -> int:
            return sum(
                1 for e in self.graph.edges if e.source_node == n or e.dest_node == n
            )

        # Select highest-degree node (ties broken by name for determinism)
        hub = max(candidates, key=lambda n: (node_degree(n), n.get_text_representer()))

        # Fix the hub for subsequent sentences
        if self._fixed_hub is None:
            self._fixed_hub = hub

        # Track current hub for visualization (blue node)
        self._current_hub = hub

        logging.info(
            "INVARIANT 3.0: Hub node selected: '%s' (degree=%d)",
            hub.get_text_representer(),
            node_degree(hub),
        )
        return hub

    def get_hub_node_name(self) -> Optional[str]:
        """
        Get the current hub node's text representation for visualization.

        Per Invariant 3.0: The hub node is visually represented as a blue node.
        """
        if self._current_hub is not None:
            return self._current_hub.get_text_representer()
        if self._fixed_hub is not None:
            return self._fixed_hub.get_text_representer()
        return None

    def _ensure_explicit_backbone_connected(
        self,
        explicit_nodes: List[Node],
        sentence_text: str,
    ) -> Tuple[List[Edge], List[Tuple[str, str, str]]]:
        added_edges: List[Edge] = []
        failed_connections: List[Tuple[str, str, str]] = []

        if len(explicit_nodes) < 2:
            return added_edges, failed_connections
        if not self.strict_attachament_constraint:
            return added_edges, failed_connections
        hub = self._select_hub_node(explicit_nodes)
        if hub is None:
            return added_edges, failed_connections
        explicit_set = set(explicit_nodes)

        def is_connected_to_hub(node: Node, visited: set) -> bool:
            if node == hub:
                return True
            if node in visited:
                return False
            visited.add(node)
            for edge in self.graph.edges:
                if edge.source_node == node and edge.dest_node in explicit_set:
                    if is_connected_to_hub(edge.dest_node, visited):
                        return True
                if edge.dest_node == node and edge.source_node in explicit_set:
                    if is_connected_to_hub(edge.source_node, visited):
                        return True
            return False

        for node in explicit_nodes:
            if node == hub:
                continue
            if is_connected_to_hub(node, set()):
                continue
            subj = node.get_text_representer()
            obj = hub.get_text_representer()
            explicit_node_names = [n.get_text_representer() for n in explicit_nodes]

            label = None
            explanation = ""

            # Attempt 1: Standard LLM call
            try:
                result = self.client.get_edge_label_with_explanation(
                    subj, obj, sentence_text, explicit_node_names, self.persona
                )
                label = result.get("label", "")
                explanation = result.get("explanation", "")
            except Exception as e:
                logging.warning(
                    "Hub edge %s -> %s: first attempt failed with LLM exception: %s",
                    subj,
                    obj,
                    str(e)[:50],
                )

            # Use relaxed validation for hub edges (accepts verbs and adjectives per 3.2)
            if label and self._is_valid_hub_edge_label(label):
                pass  # Label is valid, proceed
            else:
                # Attempt 2: Retry if first attempt failed
                logging.info(
                    "Hub edge %s -> %s: retrying (label was: '%s')",
                    subj,
                    obj,
                    label,
                )
                try:
                    result = self.client.get_edge_label_with_explanation(
                        subj, obj, sentence_text, explicit_node_names, self.persona
                    )
                    label = result.get("label", "")
                    explanation = result.get("explanation", "")
                except Exception as e:
                    logging.warning(
                        "Hub edge %s -> %s: retry also failed with exception: %s",
                        subj,
                        obj,
                        str(e)[:50],
                    )

            # INVARIANT 3.4: Failure Is Explicit, Never Silent
            # If no valid label after retries, emit diagnostic and skip (no fabricated edges)
            if not label or not self._is_valid_hub_edge_label(label):
                failure_reason = (
                    f"invalid label after retry: '{label}'"
                    if label
                    else "no label generated"
                )
                failed_connections.append((subj, obj, failure_reason))
                # Store for explicit diagnostics per 3.4
                self._connectivity_failures.append((subj, obj, failure_reason))
                logging.error(
                    "INVARIANT 3.4 VIOLATION: Explicit node '%s' cannot connect to hub '%s'. "
                    "Reason: %s. No fabricated edge created.",
                    subj,
                    obj,
                    failure_reason,
                )
                continue

            edge = self._add_edge(node, hub, label, self.edge_forget)
            if edge:
                added_edges.append(edge)
                if explanation:
                    self._hub_edge_explanations.append(
                        f"{subj} -> {obj}: {explanation}"
                    )
                logging.info(
                    "Hub-anchored edge: %s -[%s]-> %s (hub) | Explanation: %s",
                    subj,
                    label,
                    obj,
                    explanation,
                )
            else:
                failed_connections.append((subj, obj, "edge rejected by _add_edge"))
                self._connectivity_failures.append(
                    (subj, obj, "edge rejected by _add_edge")
                )
                logging.warning(
                    "FAILED hub edge %s -> %s: rejected by _add_edge validation",
                    subj,
                    obj,
                )

        # INVARIANT 3.4: Emit explicit diagnostic summary for all failures
        if failed_connections:
            logging.error(
                "INVARIANT 3.3 VIOLATION: Backbone connection FAILURES (%d/%d explicit nodes disconnected): %s",
                len(failed_connections),
                len(explicit_nodes) - 1,  # Exclude hub from count
                "; ".join(f"{s}->{h}: {r}" for s, h, r in failed_connections),
            )

        return added_edges, failed_connections

    def verify_connectivity_invariants(
        self,
        explicit_nodes: List[Node],
        sentence_index: int,
    ) -> List[Tuple[str, str, str]]:
        """
        Post-processing verification of connectivity invariants per Section 3.3-3.4.

        INVARIANT 3.3: Every explicit node must have at least one labeled edge
        connecting it to a hub node.

        INVARIANT 3.4: If connectivity fails, emit explicit diagnostics.

        Returns list of (node_name, hub_name, reason) for any disconnected nodes.
        """
        if len(explicit_nodes) < 2:
            return []

        hub = self._select_hub_node(explicit_nodes)
        if hub is None:
            logging.error(
                "INVARIANT 3.0 VIOLATION: No hub node for sentence %d",
                sentence_index,
            )
            return [("N/A", "N/A", "No hub node could be selected")]

        explicit_set = set(explicit_nodes)
        hub_name = hub.get_text_representer()
        disconnected: List[Tuple[str, str, str]] = []

        # Build graph of explicit node connections
        G = nx.Graph()
        for node in explicit_nodes:
            G.add_node(node)
        for edge in self.graph.edges:
            if edge.source_node in explicit_set and edge.dest_node in explicit_set:
                G.add_edge(edge.source_node, edge.dest_node)

        # Check connectivity to hub for each explicit node
        for node in explicit_nodes:
            if node == hub:
                continue
            if not G.has_node(node) or not nx.has_path(G, node, hub):
                node_name = node.get_text_representer()
                disconnected.append(
                    (node_name, hub_name, "No path to hub in explicit backbone")
                )

        # INVARIANT 3.4: Emit explicit diagnostics
        if disconnected:
            logging.error(
                "INVARIANT 3.3 VERIFICATION FAILED for sentence %d: "
                "%d explicit nodes not connected to hub '%s': %s",
                sentence_index,
                len(disconnected),
                hub_name,
                ", ".join(n for n, _, _ in disconnected),
            )

        return disconnected

    def _filter_inferred_for_attachment(
        self,
        relationship: Tuple[str, str, str],
        explicit_nodes: List[Node],
        explicit_words: List[str],
    ) -> bool:
        if not self.strict_attachament_constraint:
            return True
        subj, rel, obj = relationship
        explicit_lemma_keys = {tuple(n.lemmas) for n in explicit_nodes}
        explicit_word_set = {w.lower() for w in explicit_words}

        def _lemma_key(text: str) -> tuple[str, ...]:
            return tuple(get_concept_lemmas(self.spacy_nlp, text))

        subj_key = _lemma_key(subj)
        obj_key = _lemma_key(obj)
        subj_is_explicit = (
            subj.lower() in explicit_word_set or subj_key in explicit_lemma_keys
        )
        obj_is_explicit = (
            obj.lower() in explicit_word_set or obj_key in explicit_lemma_keys
        )
        return subj_is_explicit or obj_is_explicit

    def _build_per_sentence_view(
        self,
        explicit_nodes: List[Node],
        max_distance: int,
    ) -> Tuple[set[Node], set[Edge]]:
        explicit_set = set(explicit_nodes)
        distances = self._distances_from_sources_active_edges(
            explicit_set, max_distance
        )
        active_nodes = set(distances.keys())
        active_edges: set[Edge] = set()
        for edge in self.graph.edges:
            if not edge.active:
                continue
            if edge.source_node in active_nodes and edge.dest_node in active_nodes:
                active_edges.add(edge)
        return active_nodes, active_edges

    def _validate_per_sentence_connectivity(
        self,
        active_nodes: set[Node],
        active_edges: set[Edge],
        sentence_id: int,
    ) -> bool:
        if len(active_nodes) <= 1:
            return True
        G = nx.Graph()
        for node in active_nodes:
            G.add_node(node)
        for edge in active_edges:
            G.add_edge(edge.source_node, edge.dest_node)
        if not nx.is_connected(G):
            logging.error(
                "Per-sentence graph DISCONNECTED at sentence %d (strict mode)",
                sentence_id,
            )
            return False
        return True

    # =========================================================================
    # END REDESIGNED STRICT ATTACHMENT CONSTRAINT
    # =========================================================================

    def resolve_pronouns(self, text: str) -> str:
        resolved = self.client.resolve_pronouns(text, self.persona)
        if not isinstance(resolved, str) or not resolved.strip():
            return text
        low = resolved.lower()
        if "does not mention any pronouns" in low or "no pronouns to replace" in low:
            return text
        return resolved

    def _graph_edges_to_triplets(
        self, only_active: bool = False
    ) -> List[Tuple[str, str, str]]:
        triplets: List[Tuple[str, str, str]] = []
        for edge in self.graph.edges:
            if only_active and not edge.active:
                continue
            if not edge.label or not str(edge.label).strip():
                continue
            if edge.source_node == edge.dest_node:
                continue
            triplets.append(
                (
                    edge.source_node.get_text_representer(),
                    edge.label,
                    edge.dest_node.get_text_representer(),
                )
            )
        return triplets

    # Step 5 from paper - only explicit nodes from the current sentence stay active
    def _restrict_active_to_current_explicit(self, explicit_nodes: List[Node]) -> None:
        explicit_set = set(explicit_nodes)
        inactive_score = self.max_distance_from_active_nodes + 1
        for node in self.graph.nodes:
            if node in explicit_set and node.node_source == NodeSource.TEXT_BASED:
                node.score = 0
            else:
                node.score = inactive_score

    def _get_nodes_with_active_edges(self) -> set[Node]:
        active_nodes: set[Node] = set()
        for edge in self.graph.edges:
            if edge.active:
                active_nodes.add(edge.source_node)
                active_nodes.add(edge.dest_node)
        # Ensure explicit nodes are always considered attachable.
        return active_nodes | getattr(self, "_explicit_nodes_current_sentence", set())

    def _can_attach(self, node: Node) -> bool:
        attachable = (
            self._get_nodes_with_active_edges()
            | self._anchor_nodes
            | getattr(self, "_explicit_nodes_current_sentence", set())
        )
        return node in attachable

    def _normalize_label(self, label: str) -> str:
        norm = (label or "").strip().lower()
        norm = re.sub(r"[\s\-]+", "_", norm)
        return norm

    def _edge_key(self, edge: Edge) -> tuple[str, str, str]:
        return (
            edge.source_node.get_text_representer(),
            edge.dest_node.get_text_representer(),
            edge.label,
        )

    def _record_edge_in_graphs(self, edge: Edge, sentence_idx: Optional[int]) -> None:
        u, v, lbl = self._edge_key(edge)
        # Safety check: skip recording edges with empty/whitespace labels
        if not lbl or not lbl.strip():
            logging.warning(
                "Skipping recording edge with empty label: %s -> %s",
                u,
                v,
            )
            return
        introduced = self._triplet_intro.get((u, lbl, v))
        if introduced is None:
            introduced = (
                edge.created_at_sentence if edge.created_at_sentence is not None else -1
            )
        self._triplet_intro[(u, lbl, v)] = int(introduced)
        edge_key = f"{lbl}__introduced_{introduced}"

        # Append-only cumulative record: one row per state observation when we touch an edge.
        if sentence_idx is not None:
            self._cumulative_triplet_records.append(
                {
                    "subject": u,
                    "relation": lbl,
                    "object": v,
                    "introduced_at": int(introduced),
                    "last_active": int(sentence_idx),
                    "currently_active": bool(edge.active),
                }
            )

        # Active projection (salience)
        if edge.active:
            self.active_graph.add_edge(
                u,
                v,
                key=edge_key,
                relation=lbl,
                introduced_at_sentence=int(introduced),
                last_active_sentence=int(
                    sentence_idx if sentence_idx is not None else -1
                ),
            )
        else:
            if self.active_graph.has_edge(u, v, key=edge_key):
                try:
                    self.active_graph.remove_edge(u, v, key=edge_key)
                except Exception:
                    pass

    def _graph_to_triplets(self, graph: nx.MultiDiGraph) -> List[Tuple[str, str, str]]:
        trips: List[Tuple[str, str, str]] = []
        for u, v, key, data in graph.edges(keys=True, data=True):
            rel = data.get("relation") or key
            # Skip edges with empty/whitespace-only labels
            if not rel or not str(rel).strip():
                continue
            trips.append((u, rel, v))
        return trips

    def _cumulative_triplets_upto(
        self, sentence_idx: Optional[int] = None
    ) -> List[Tuple[str, str, str]]:
        trips = []
        for edge in self.graph.edges:
            if not edge.label:
                continue
            trips.append(
                (
                    edge.source_node.get_text_representer(),
                    edge.label,
                    edge.dest_node.get_text_representer(),
                )
            )
        return trips

    def _is_generic_relation(self, label: str) -> bool:
        norm = self._normalize_label(label)
        return norm in self.GENERIC_RELATION_LABELS

    def _is_blacklisted_relation(self, label: str) -> bool:
        norm = self._normalize_label(label)
        return norm in self.RELATION_BLACKLIST

    def _is_verb_or_adjective_relation(self, label: str) -> bool:
        doc = self.spacy_nlp(label)
        has_verb_or_adj = False
        for tok in doc:
            if not getattr(tok, "is_alpha", False):
                continue
            # Accept VERB, AUX (auxiliary verbs like 'is'), or ADJ
            if tok.pos_ in {"VERB", "AUX", "ADJ"}:
                has_verb_or_adj = True
            # Reject NOUN, PROPN, ADV as standalone relations
            elif tok.pos_ in {"NOUN", "PROPN", "ADV"}:
                return False
        return has_verb_or_adj

    def _is_valid_relation_label(self, label: str) -> bool:
        """
        Validate edge labels per Invariant 3.2.

        Edges may be verbs or adjectives (e.g., 'is_young', 'rides_through').
        Unlabeled or purely nominal edges are forbidden.
        """
        # Explicitly handle None, empty string, and whitespace-only labels
        if not label or not isinstance(label, str):
            return False
        label_stripped = label.strip()
        if not label_stripped:
            return False
        if self._is_generic_relation(label_stripped):
            return False
        if self._is_blacklisted_relation(label_stripped):
            return False
        # Per 3.2: Accept verb OR adjective relations
        if not self._is_verb_or_adjective_relation(label_stripped):
            return False
        return True

    def _is_valid_hub_edge_label(self, label: str) -> bool:
        """
        Relaxed validation for hub-anchoring edges per Invariants 3.2 and 3.3.

        Hub edges connect explicit nodes to the central hub node to ensure
        graph connectivity. We accept:
        - Verb labels (action verbs, auxiliary verbs)
        - Adjectival labels (e.g., 'is_young', 'is_fortified')
        - Labels from the hub-edge acceptable set

        Per 3.2: Adjectival edges count toward connectivity guarantees.
        """
        if not label or not isinstance(label, str):
            return False
        label_stripped = label.strip()
        if not label_stripped:
            return False

        # Normalize for comparison
        norm = self._normalize_label(label_stripped)

        # Accept labels from the hub-edge acceptable set
        if norm in self.HUB_EDGE_ACCEPTABLE_LABELS:
            return True

        # Accept labels with a verb (AUX or VERB) or adjective
        doc = self.spacy_nlp(label_stripped)
        has_verb_or_adj = any(
            tok.pos_ in {"VERB", "AUX", "ADJ"}
            for tok in doc
            if getattr(tok, "is_alpha", False)
        )
        if has_verb_or_adj:
            return True

        # Reject everything else
        return False

    def _normalize_endpoint_text(self, text: str, is_subject: bool) -> Optional[str]:
        if not text:
            return None
        doc = self.spacy_nlp(text)
        if not doc:
            return None
        allowed_subject = {"NOUN", "PROPN", "PRON"}
        allowed_object = {"NOUN", "PROPN", "PRON", "ADJ"}
        for tok in doc:
            if not getattr(tok, "is_alpha", False):
                continue
            pos = tok.pos_
            if is_subject and pos not in allowed_subject:
                continue
            if not is_subject and pos not in allowed_object:
                continue
            lemma = (getattr(tok, "lemma_", "") or "").strip().lower()
            if not lemma or lemma in self.spacy_nlp.Defaults.stop_words:
                continue
            return lemma
        return None

    def _has_edge_between(self, a: Node, b: Node) -> bool:
        for edge in self.graph.edges:
            if (edge.source_node == a and edge.dest_node == b) or (
                edge.source_node == b and edge.dest_node == a
            ):
                return True
        return False

    def _find_node_by_text(
        self, text: str, candidates: Iterable[Node]
    ) -> Optional[Node]:
        canon = canonicalize_node_text(self.spacy_nlp, text)
        lemmas = tuple(get_concept_lemmas(self.spacy_nlp, canon))
        for node in candidates:
            if lemmas == tuple(node.lemmas):
                return node
        return None

    def _appears_in_story(self, text: str) -> bool:
        if not text:
            return False
        doc = self.spacy_nlp(text)
        return any(
            tok.lemma_.lower() in self.story_lemmas for tok in doc if tok.is_alpha
        )

    def _classify_canonical_node_text(self, canon: str) -> Optional[NodeType]:
        if not canon:
            return None
        doc = self.spacy_nlp(canon)
        if not doc:
            return None
        token = next((t for t in doc if getattr(t, "is_alpha", False)), None) or doc[0]
        lemma = (getattr(token, "lemma_", "") or "").lower()
        if lemma in self.spacy_nlp.Defaults.stop_words:
            return None
        if token.pos_ in {"NOUN", "PROPN"}:
            return NodeType.CONCEPT
        if token.pos_ == "ADJ":
            return NodeType.PROPERTY
        return None

    def _canonicalize_and_classify_node_text(
        self, text: str
    ) -> tuple[str, Optional[NodeType]]:
        canon = canonicalize_node_text(self.spacy_nlp, text)
        return canon, self._classify_canonical_node_text(canon)

    def _plot_graph_snapshot(
        self,
        sentence_index: int,
        sentence_text: str,
        output_dir: Optional[str],
        highlight_nodes: Optional[Iterable[str]],
        only_active: bool = False,
        largest_component_only: bool = False,
        mode: str = "sentence_active",
        triplets_override: Optional[List[Tuple[str, str, str]]] = None,
        active_edges: Optional[set[tuple[str, str]]] = None,
        explicit_nodes: Optional[List[str]] = None,
        salient_nodes: Optional[List[str]] = None,
        inactive_nodes: Optional[List[str]] = None,
        hub_edge_explanations: Optional[List[str]] = None,
    ) -> None:
        # Route per-sentence plots into mode-specific subfolders for clarity.
        plot_dir = output_dir
        if output_dir and mode in {"sentence_active", "sentence_cumulative"}:
            subdir = "active" if mode == "sentence_active" else "cummulative"
            plot_dir = os.path.join(output_dir, subdir)
        triplets = (
            triplets_override
            if triplets_override is not None
            else self._graph_edges_to_triplets(only_active=only_active)
        )
        age_for_filename = self.persona_age if self.persona_age is not None else -1

        # Per Invariant 3.0: Hub node must be visually represented as blue
        blue_node_set = set(highlight_nodes) if highlight_nodes else set()
        hub_name = self.get_hub_node_name()
        if hub_name:
            blue_node_set.add(hub_name)

        try:
            saved_path = plot_amoc_triplets(
                triplets=(
                    self._graph_edges_to_triplets(only_active=True)
                    if only_active
                    else triplets
                ),
                persona=self.persona,
                model_name=self.model_name,
                age=age_for_filename,
                blue_nodes=blue_node_set,
                output_dir=plot_dir,
                step_tag=(
                    f"sent{sentence_index+1}_{mode}"
                    if mode
                    else f"sent{sentence_index+1}"
                ),
                sentence_text=sentence_text,
                inactive_nodes=inactive_nodes,
                explicit_nodes=explicit_nodes,
                salient_nodes=salient_nodes,
                largest_component_only=largest_component_only,
                positions=self._viz_positions,
                active_edges=active_edges,
                hub_edge_explanations=hub_edge_explanations,
            )
            if triplets:
                logging.info(
                    "[Plot] Saved sentence %d graph to %s", sentence_index, saved_path
                )
            else:
                logging.info(
                    "[Plot] Sentence %d graph skipped (no active edges)", sentence_index
                )
        except Exception:
            logging.error("Failed to plot graph snapshot", exc_info=True)

    def analyze(
        self,
        replace_pronouns: bool = True,
        plot_after_each_sentence: bool = False,
        graphs_output_dir: Optional[str] = None,
        highlight_nodes: Optional[Iterable[str]] = None,
        matrix_suffix: Optional[str] = None,
        largest_component_only: bool = False,
    ) -> List[Tuple[str, str, str]]:
        if not hasattr(self, "_amoc_matrix_records"):
            self._amoc_matrix_records = []
        # Initialize persistent visualization positions ONCE per analyze run.
        # These positions must remain stable across sentences.
        if not hasattr(self, "_viz_positions") or self._viz_positions is None:
            self._viz_positions = {}
        self._cumulative_deactivated_nodes_for_plot = set()
        self._prev_active_nodes_for_plot = set()

        def _clean_resolved_sentence(orig_text: str, candidate: str) -> str:
            if not isinstance(candidate, str) or not candidate.strip():
                return orig_text
            cleaned = re.sub(r"<[^>]+>", " ", candidate)
            cleaned = re.sub(r"\s+", " ", cleaned).strip()

            # Pick the candidate sentence with the highest content-word overlap
            # with the original to avoid prompt echoes in the header.
            orig_doc = self.spacy_nlp(orig_text)
            orig_tokens = {t.lemma_.lower() for t in orig_doc if t.is_alpha}
            best_sent = None
            best_overlap = -1
            cand_doc = self.spacy_nlp(cleaned)
            for sent in cand_doc.sents:
                toks = {t.lemma_.lower() for t in sent if t.is_alpha}
                overlap = len(orig_tokens & toks)
                if overlap > best_overlap:
                    best_overlap = overlap
                    best_sent = sent.text.strip()

            chosen = best_sent or cleaned

            # Trim runaway echoes to a reasonable length.
            max_len = max(len(orig_text) * 2 + 40, 400)
            if len(chosen) > max_len:
                chosen = chosen[:max_len].rstrip(" ,.;") + "..."
            return chosen or orig_text

        doc = self.spacy_nlp(self.story_text)
        resolved_sentences: list[tuple[Span, str, str]] = []
        for orig_sent in doc.sents:
            resolved_text = orig_sent.text
            if replace_pronouns:
                candidate = self.resolve_pronouns(orig_sent.text)
                if isinstance(candidate, str) and candidate.strip():
                    resolved_text = _clean_resolved_sentence(orig_sent.text, candidate)
            resolved_doc = self.spacy_nlp(resolved_text)
            if not resolved_doc:
                resolved_text = orig_sent.text
                resolved_doc = self.spacy_nlp(resolved_text)
            resolved_span = resolved_doc[0 : len(resolved_doc)]
            resolved_sentences.append((resolved_span, resolved_text, orig_sent.text))

        prev_sentences: list[str] = []
        current_sentence = ""
        self._sentence_triplets: list[
            tuple[int, str, str, str, str, bool, bool, int]
        ] = (
            []
        )  # sentence_idx, sentence_text, subj, rel, obj, active, anchor_kept, introduced_at
        for i, (sent, resolved_text, original_text) in enumerate(resolved_sentences):
            # Working-memory projection is rebuilt each sentence.
            self.active_graph = nx.MultiDiGraph()
            self._current_sentence_index = i + 1
            self._current_sentence_text = original_text
            self._anchor_drop_log: list[tuple[int, str, str, str, str]] = (
                []
            )  # sent_idx, sent_text, subj, rel, obj
            nodes_before_sentence = set(self.graph.nodes)
            self._explicit_nodes_current_sentence = set()
            self._hub_edge_explanations = []  # Clear explanations for new sentence
            logging.info("Processing sentence %d: %s", i, resolved_text)
            if i == 0:
                current_sentence = sent
                prev_sentences.append(resolved_text)

                # ============================================================
                # STEP 1: Build Explicit Backbone (Section 3.1.2 Steps 1a-1c)
                # ============================================================
                self.init_graph(sent)

                (
                    current_sentence_text_based_nodes,
                    current_sentence_text_based_words,
                ) = self.get_senteces_text_based_nodes(
                    [sent], create_unexistent_nodes=True
                )
                self._explicit_nodes_current_sentence = set(
                    current_sentence_text_based_nodes
                )
                # Populate _anchor_nodes from first sentence's explicit nodes
                self._anchor_nodes = set(current_sentence_text_based_nodes)

                # Ensure explicit backbone is connected (hub-anchored if needed)
                if self.strict_attachament_constraint:
                    backbone_failures: List[Tuple[str, str, str]] = []
                    if not self._is_explicit_backbone_connected(
                        current_sentence_text_based_nodes
                    ):
                        _, backbone_failures = self._ensure_explicit_backbone_connected(
                            current_sentence_text_based_nodes,
                            resolved_text,
                        )
                    # Validate Step 1 result
                    if not self._is_explicit_backbone_connected(
                        current_sentence_text_based_nodes
                    ):
                        failure_detail = (
                            f" Failures: {backbone_failures}"
                            if backbone_failures
                            else ""
                        )
                        logging.error(
                            "Step 1 FAILED: Explicit backbone disconnected at sentence 1.%s",
                            failure_detail,
                        )

                # ============================================================
                # STEP 2: Inference Enrichment (Section 3.1.2 Steps 2a-2c)
                # ============================================================
                inferred_concept_relationships, inferred_property_relationships = (
                    self.infer_new_relationships_step_0(sent)
                )

                self.add_inferred_relationships_to_graph_step_0(
                    inferred_concept_relationships,
                    NodeType.CONCEPT,
                    sent,
                    current_sentence_text_based_nodes,
                    current_sentence_text_based_words,
                )
                self.add_inferred_relationships_to_graph_step_0(
                    inferred_property_relationships,
                    NodeType.PROPERTY,
                    sent,
                    current_sentence_text_based_nodes,
                    current_sentence_text_based_words,
                )

                # Validate connectivity (no post-hoc repair in strict mode)
                if self.strict_attachament_constraint:
                    self._enforce_graph_connectivity()
                else:
                    # Legacy mode: still do post-hoc cleanup
                    self._enforce_graph_connectivity()

                self._restrict_active_to_current_explicit(
                    current_sentence_text_based_nodes
                )
                self.graph.set_nodes_score_based_on_distance_from_active_nodes(
                    current_sentence_text_based_nodes
                )
            else:
                added_edges = []
                current_sentence = sent
                prev_sentences.append(resolved_text)
                if len(prev_sentences) > self.context_length:
                    prev_sentences.pop(0)

                current_sentence_text_based_nodes, current_sentence_text_based_words = (
                    self.get_senteces_text_based_nodes(
                        [current_sentence], create_unexistent_nodes=True
                    )
                )
                self._explicit_nodes_current_sentence = set(
                    current_sentence_text_based_nodes
                )

                current_all_text = resolved_text
                # Step 3: build active subgraph using only explicit (text-based) nodes.
                graph_active_nodes = self.graph.get_active_nodes(
                    self.max_distance_from_active_nodes, only_text_based=True
                )
                active_nodes_text = self.graph.get_nodes_str(graph_active_nodes)
                active_nodes_edges_text, _ = self.graph.get_edges_str(
                    graph_active_nodes, only_text_based=True
                )

                nodes_from_text = ""
                for idx, node in enumerate(current_sentence_text_based_nodes):
                    nodes_from_text += f" - ({current_sentence_text_based_words[idx]}, {node.node_type})\n"

                new_relationships = self.client.get_new_relationships(
                    nodes_from_text,  # 1. Nodes from Text
                    active_nodes_text,  # 2. Nodes from Graph (explicit only)
                    active_nodes_edges_text,  # 3. Edges from Graph (explicit only)
                    current_all_text,  # 4. Text
                    self.persona,  # 5. Persona
                )

                text_based_activated_nodes = current_sentence_text_based_nodes
                sentence_lemma_keys = {
                    tuple(n.lemmas) for n in current_sentence_text_based_nodes
                }
                for idx, relationship in enumerate(new_relationships):
                    # Skip None or scalar junk (int, float, bool, etc.)
                    if relationship is None or isinstance(
                        relationship, (int, float, bool)
                    ):
                        logging.error(
                            f"[AMoC] Skipping non-iterable relationship at {idx}: {relationship!r}"
                        )
                        continue

                    # If relationship is a dict, try to convert it
                    if isinstance(relationship, dict):
                        subj = relationship.get("subject") or relationship.get("head")
                        rel = relationship.get("relation") or relationship.get(
                            "predicate"
                        )
                        obj = relationship.get("object") or relationship.get("tail")
                        if not (subj and rel and obj):
                            logging.error(
                                f"[AMoC] Skipping malformed dict relationship at {idx}: {relationship!r}"
                            )
                            continue
                        relationship = (str(subj), str(rel), str(obj))

                    # Must be list/tuple from this point on
                    if not isinstance(relationship, (list, tuple)):
                        logging.error(
                            f"[AMoC] Skipping unexpected relationship type at {idx}: {type(relationship)}  {relationship!r}"
                        )
                        continue

                    # Must have exactly 3 elements
                    if len(relationship) != 3:
                        logging.error(
                            f"[AMoC] Skipping relationship with wrong length at {idx}: {relationship!r}"
                        )
                        continue

                    # Unpack
                    subj, rel, obj = relationship

                    subj = self._normalize_endpoint_text(subj, is_subject=True) or None
                    obj = self._normalize_endpoint_text(obj, is_subject=False) or None
                    if subj is None or obj is None:
                        continue
                    # Validate subject/object strings
                    if not subj or not obj:
                        continue
                    if subj == obj:
                        continue
                    if not isinstance(subj, str) or not isinstance(obj, str):
                        continue

                    if not self._passes_attachment_constraint(
                        subj,
                        obj,
                        current_sentence_text_based_words,
                        current_sentence_text_based_nodes,
                        graph_active_nodes,
                        self._get_nodes_with_active_edges(),
                    ):
                        continue

                    # Continue with your original code
                    source_node = self.get_node_from_new_relationship(
                        subj,
                        graph_active_nodes,
                        current_sentence_text_based_nodes,
                        current_sentence_text_based_words,
                        node_source=NodeSource.TEXT_BASED,
                        create_node=True,
                    )

                    dest_node = self.get_node_from_new_relationship(
                        obj,
                        graph_active_nodes,
                        current_sentence_text_based_nodes,
                        current_sentence_text_based_words,
                        node_source=NodeSource.TEXT_BASED,
                        create_node=True,
                    )
                    edge_label = rel.replace("(edge)", "").strip()
                    if not self._is_valid_relation_label(edge_label):
                        continue
                    if source_node is None or dest_node is None:
                        continue

                    if tuple(source_node.lemmas) in sentence_lemma_keys:
                        source_node.node_source = NodeSource.TEXT_BASED
                    if tuple(dest_node.lemmas) in sentence_lemma_keys:
                        dest_node.node_source = NodeSource.TEXT_BASED

                    potential_new_edge = self._add_edge(
                        source_node, dest_node, edge_label, self.edge_forget
                    )
                    if potential_new_edge:
                        added_edges.append(potential_new_edge)

                # =====================================================
                # STEP 1 VALIDATION: Ensure explicit backbone is connected
                # =====================================================
                if self.strict_attachament_constraint:
                    backbone_failures: List[Tuple[str, str, str]] = []
                    if not self._is_explicit_backbone_connected(
                        current_sentence_text_based_nodes
                    ):
                        hub_edges, backbone_failures = (
                            self._ensure_explicit_backbone_connected(
                                current_sentence_text_based_nodes,
                                resolved_text,
                            )
                        )
                        added_edges.extend(hub_edges)
                    # Validate Step 1 result
                    if not self._is_explicit_backbone_connected(
                        current_sentence_text_based_nodes
                    ):
                        failure_detail = (
                            f" Failures: {backbone_failures}"
                            if backbone_failures
                            else ""
                        )
                        logging.error(
                            "Step 1 FAILED: Explicit backbone disconnected at sentence %d.%s",
                            i + 1,
                            failure_detail,
                        )

                # =====================================================
                # STEP 2: Inference Enrichment (Section 3.1.2 Steps 2a-2c)
                # Inferences MUST attach to explicit backbone
                # =====================================================
                inferred_concept_relationships, inferred_property_relationships = (
                    self.infer_new_relationships(
                        current_all_text,
                        current_sentence_text_based_nodes,
                        current_sentence_text_based_words,
                        self.graph.get_nodes_str(
                            self.graph.get_active_nodes(
                                self.max_distance_from_active_nodes,
                                only_text_based=True,
                            )
                        ),
                        self.graph.get_edges_str(
                            self.graph.get_active_nodes(
                                self.max_distance_from_active_nodes,
                                only_text_based=True,
                            ),
                            only_text_based=True,
                        )[0],
                    )
                )

                self.add_inferred_relationships_to_graph(
                    inferred_concept_relationships,
                    NodeType.CONCEPT,
                    current_sentence_text_based_nodes,
                    current_sentence_text_based_words,
                    graph_active_nodes,
                    added_edges,
                )
                self.add_inferred_relationships_to_graph(
                    inferred_property_relationships,
                    NodeType.PROPERTY,
                    current_sentence_text_based_nodes,
                    current_sentence_text_based_words,
                    graph_active_nodes,
                    added_edges,
                )

                if self.ENFORCE_ATTACHMENT_CONSTRAINT:
                    targeted_edges = self._infer_edges_to_recently_deactivated(
                        current_sentence_text_based_nodes,
                        current_sentence_text_based_words,
                        current_all_text,
                    )
                    added_edges.extend(targeted_edges)

                self.graph.set_nodes_score_based_on_distance_from_active_nodes(
                    text_based_activated_nodes
                )
                self.reactivate_relevant_edges(
                    self.graph.get_active_nodes(
                        self.max_distance_from_active_nodes, only_text_based=True
                    ),
                    " ".join(prev_sentences),
                    added_edges,
                )
                self._restrict_active_to_current_explicit(
                    current_sentence_text_based_nodes
                )
                self.graph.set_nodes_score_based_on_distance_from_active_nodes(
                    current_sentence_text_based_nodes
                )
                # Update anchor nodes to include current explicit nodes and
                # nodes with active edges to maintain connectivity across sentences
                self._anchor_nodes = (
                    self._anchor_nodes
                    | set(current_sentence_text_based_nodes)
                    | self._get_nodes_with_active_edges()
                )

            if self.debug:
                logging.info(
                    "Active graph after sentence %d:\n%s",
                    i,
                    self.graph.get_active_graph_repr(),
                )

            sentence_id = i + 1
            newly_inferred_nodes = {
                n
                for n in (set(self.graph.nodes) - nodes_before_sentence)
                if n.node_source == NodeSource.INFERENCE_BASED
            }
            # Refresh active projection for this step
            self._record_sentence_activation(
                sentence_id=sentence_id,
                explicit_nodes=current_sentence_text_based_nodes,
                newly_inferred_nodes=newly_inferred_nodes,
            )

            current_active_nodes = self._get_nodes_with_active_edges()
            if self.active_graph.number_of_nodes() > 1 and not nx.is_connected(
                nx.Graph(self.active_graph)
            ):
                logging.error(
                    "Active graph disconnected at sentence %s for persona '%s'",
                    sentence_id,
                    self.persona,
                )
            if i == 0:
                recently_deactivated_nodes: set[Node] = set()
            else:
                appeared = current_active_nodes - self._prev_active_nodes_for_plot
                gone = self._prev_active_nodes_for_plot - current_active_nodes
                self._cumulative_deactivated_nodes_for_plot.update(gone)
                self._cumulative_deactivated_nodes_for_plot.difference_update(
                    current_active_nodes
                )
                recently_deactivated_nodes = set(gone)
            inactive_nodes_for_plot = sorted(
                filter(
                    None,
                    {
                        node.get_text_representer()
                        for node in self._cumulative_deactivated_nodes_for_plot
                    },
                )
            )
            explicit_nodes_for_plot = sorted(
                filter(
                    None,
                    {
                        node.get_text_representer()
                        for node in current_sentence_text_based_nodes
                    },
                )
            )
            salient_nodes_for_plot = sorted(
                {
                    node.get_text_representer()
                    for node in current_active_nodes
                    if node.get_text_representer()
                }
                - set(explicit_nodes_for_plot)
            )
            # Only keep the deactivated set for targeted inference when the
            # attachment constraint is enforced.
            if self.ENFORCE_ATTACHMENT_CONSTRAINT:
                self._recently_deactivated_nodes_for_inference = (
                    recently_deactivated_nodes
                )
            else:
                self._recently_deactivated_nodes_for_inference = set()
            self._prev_active_nodes_for_plot = current_active_nodes

            if plot_after_each_sentence:
                # Active (salience) view
                self._plot_graph_snapshot(
                    sentence_index=i,
                    sentence_text=sent.text,
                    output_dir=graphs_output_dir,
                    highlight_nodes=highlight_nodes,
                    inactive_nodes=inactive_nodes_for_plot,
                    explicit_nodes=explicit_nodes_for_plot,
                    salient_nodes=salient_nodes_for_plot,
                    only_active=True,
                    largest_component_only=largest_component_only,
                    mode="sentence_active",
                    triplets_override=self._graph_to_triplets(self.active_graph),
                    active_edges={(u, v) for u, v in self.active_graph.edges()},
                    hub_edge_explanations=(
                        self._hub_edge_explanations
                        if self._hub_edge_explanations
                        else None
                    ),
                )
                # Cumulative memory view
                self._plot_graph_snapshot(
                    sentence_index=i,
                    sentence_text=sent.text,
                    output_dir=graphs_output_dir,
                    highlight_nodes=highlight_nodes,
                    inactive_nodes=inactive_nodes_for_plot,
                    explicit_nodes=explicit_nodes_for_plot,
                    salient_nodes=salient_nodes_for_plot,
                    only_active=False,
                    largest_component_only=largest_component_only,
                    mode="sentence_cumulative",
                    triplets_override=self._cumulative_triplets_upto(
                        self._current_sentence_index
                    ),
                    active_edges={
                        (
                            edge.source_node.get_text_representer(),
                            edge.dest_node.get_text_representer(),
                        )
                        for edge in self.graph.edges
                        if edge.active
                    },
                )

            # Capture triplets for this sentence (all edges, with current active flag)
            for edge in self.graph.edges:
                intro = self._triplet_intro.get(
                    (
                        edge.source_node.get_text_representer(),
                        edge.label,
                        edge.dest_node.get_text_representer(),
                    ),
                    edge.created_at_sentence if edge.created_at_sentence else -1,
                )
                self._sentence_triplets.append(
                    (
                        self._current_sentence_index,
                        original_text,
                        edge.source_node.get_text_representer(),
                        edge.label,
                        edge.dest_node.get_text_representer(),
                        edge.active,
                        True,  # anchor_kept
                        int(intro),
                    )
                )
            for sent_idx, sent_text, subj, rel, obj in getattr(
                self, "_anchor_drop_log", []
            ):
                self._sentence_triplets.append(
                    (
                        sent_idx,
                        sent_text,
                        subj,
                        rel,
                        obj,
                        False,  # inactive/not added
                        False,  # anchor_kept flag
                        -1,  # introduced_at unknown/unused for dropped anchors
                    )
                )

        # save score matrix
        df = pd.DataFrame(self._amoc_matrix_records)
        # Collapse duplicate token/sentence entries by mean to avoid pivot errors.
        if not df.empty:
            df = (
                df.groupby(["token", "sentence"], as_index=False)["score"]
                .mean()
                .astype({"token": str})
            )
        matrix = (
            df.pivot(index="token", columns="sentence", values="score")
            .sort_index()
            .fillna(0.0)
        )
        # Order rows by salience: highest peak activation first, then total activation.
        salience_max = matrix.max(axis=1)
        salience_sum = matrix.sum(axis=1)
        ordering = (
            salience_max.to_frame("max")
            .assign(sum=salience_sum)
            .sort_values(by=["max", "sum", "token"], ascending=[False, False, True])
        )
        matrix = matrix.loc[ordering.index]
        # Prepend full story text as first row for traceability.
        if len(matrix.columns) > 0:
            story_row = pd.DataFrame(
                [{col: "" for col in matrix.columns}], index=["story_text"]
            )
            story_row.iloc[0, 0] = self.story_text
            matrix = pd.concat([story_row, matrix])

        matrix_dir = os.path.join(self.matrix_dir_base, "matrix")
        os.makedirs(matrix_dir, exist_ok=True)
        safe_model = _sanitize_filename_component(self.model_name, max_len=60)
        safe_persona = _sanitize_filename_component(self.persona, max_len=60)
        age_for_filename = self.persona_age if self.persona_age is not None else -1
        suffix = (
            f"_{_sanitize_filename_component(matrix_suffix)}" if matrix_suffix else ""
        )
        matrix_filename = (
            f"amoc_matrix_{safe_model}_{safe_persona}_{age_for_filename}{suffix}.csv"
        )
        matrix_path = os.path.join(matrix_dir, matrix_filename)

        matrix.to_csv(matrix_path)
        logging.info(
            "[Matrix] Saved activation matrix for persona '%s' to %s",
            self.persona,
            matrix_path,
        )
        logging.info("AMoC activation matrix:\n%s", matrix.to_string())
        # Collect final active triplets: edges active after the final sentence.
        final_sentence_idx = getattr(self, "_current_sentence_index", None)
        final_triplets = []
        for edge in self.graph.edges:
            if not edge.active:
                continue
            subj = edge.source_node.get_text_representer()
            obj = edge.dest_node.get_text_representer()
            rel = edge.label
            intro = self._triplet_intro.get((subj, rel, obj))
            if intro is None:
                intro = edge.created_at_sentence if edge.created_at_sentence else -1
            final_triplets.append(
                (
                    subj,
                    rel,
                    obj,
                    True,
                    int(intro),
                    int(final_sentence_idx) if final_sentence_idx is not None else -1,
                )
            )

        cumulative_triplets = []
        for edge in self.graph.edges:
            subj = edge.source_node.get_text_representer()
            obj = edge.dest_node.get_text_representer()
            rel = edge.label
            intro = self._triplet_intro.get((subj, rel, obj))
            if intro is None:
                intro = edge.created_at_sentence if edge.created_at_sentence else -1
            cumulative_triplets.append((subj, rel, obj, int(intro)))

        return final_triplets, self._sentence_triplets, cumulative_triplets

    def infer_new_relationships_step_0(
        self, sent: Span
    ) -> Tuple[List[Tuple[str, str, str]], List[Tuple[str, str, str]]]:
        current_sentence_text_based_nodes, current_sentence_text_based_words = (
            self.get_senteces_text_based_nodes([sent], create_unexistent_nodes=False)
        )

        nodes_from_text = ""
        for i, node in enumerate(current_sentence_text_based_nodes):
            nodes_from_text += (
                f" - ({current_sentence_text_based_words[i]}, {node.node_type})\n"
            )

        for _ in range(3):
            try:
                object_properties_dict = (
                    self.client.infer_objects_and_properties_first_sentence(
                        nodes_from_text, sent.text, self.persona
                    )
                )
                break
            except:
                continue
        else:
            return [], []

        for _ in range(3):
            try:
                new_relationships = (
                    self.client.generate_new_inferred_relationships_first_sentence(
                        nodes_from_text,
                        object_properties_dict["concepts"][: self.max_new_concepts],
                        object_properties_dict["properties"][: self.max_new_properties],
                        sent.text,
                        self.persona,
                    )
                )
                return (
                    new_relationships["concept_relationships"],
                    new_relationships["property_relationships"],
                )
            except:
                continue
        return [], []

    def infer_new_relationships(
        self,
        text: str,
        current_sentence_text_based_nodes: List[Node],
        current_sentence_text_based_words: List[str],
        graph_nodes_representation: str,
        graph_edges_representation: str,
    ) -> Tuple[List[Tuple[str, str, str]], List[Tuple[str, str, str]]]:
        nodes_from_text = ""
        for i, node in enumerate(current_sentence_text_based_nodes):
            nodes_from_text += (
                f" - ({current_sentence_text_based_words[i]}, {node.node_type})\n"
            )

        for _ in range(3):
            try:
                object_properties_dict = self.client.infer_objects_and_properties(
                    nodes_from_text,
                    graph_nodes_representation,
                    graph_edges_representation,
                    text,
                    self.persona,
                )
                break
            except:
                continue

        for _ in range(3):
            try:
                new_relationships = self.client.generate_new_inferred_relationships(
                    nodes_from_text,
                    graph_nodes_representation,
                    graph_edges_representation,
                    object_properties_dict["concepts"][: self.max_new_concepts],
                    object_properties_dict["properties"][: self.max_new_properties],
                    text,
                    self.persona,
                )
                return (
                    new_relationships["concept_relationships"],
                    new_relationships["property_relationships"],
                )
            except:
                continue
        return [], []

    def reactivate_relevant_edges(
        self,
        active_nodes: List[Node],
        prev_sentences_text: str,
        newly_added_edges: List[Edge],
    ) -> None:
        edges_text, edges = self.graph.get_edges_str(
            self.graph.nodes, only_active=False
        )
        # Non-strict mode: accumulate salience monotonically (no fading/pruning).
        if not self.strict_reactivate_function:
            for edge in edges:
                edge.active = True
                edge.forget_score = self.edge_forget
                self._record_edge_in_graphs(edge, self._current_sentence_index)
            # Enforce connectivity even in non-strict mode
            self._enforce_graph_connectivity()
            return

        raw_indices = self.client.get_relevant_edges(
            edges_text, prev_sentences_text, self.persona
        )

        valid_indices: List[int] = []
        for idx in raw_indices:
            try:
                i = int(idx)
            except Exception:
                continue
            if 1 <= i <= len(edges):
                valid_indices.append(i)

        valid_indices = valid_indices[: self.nr_relevant_edges]
        active_node_set = set(active_nodes)
        if not valid_indices:
            # Improved fallback edge selection: keep edges that are newly added,
            # or active edges that touch at least one active/anchor node.
            # Uses only connected nodes to prevent selecting disconnected edges.
            selected = set()
            connected_nodes = active_node_set | self._anchor_nodes
            connected_lemma_keys = {tuple(n.lemmas) for n in connected_nodes}
            for idx, edge in enumerate(edges, start=1):
                if edge in newly_added_edges:
                    selected.add(idx)
                elif edge.active:
                    # Check both direct membership and lemma matching
                    source_connected = (
                        edge.source_node in connected_nodes
                        or tuple(edge.source_node.lemmas) in connected_lemma_keys
                    )
                    dest_connected = (
                        edge.dest_node in connected_nodes
                        or tuple(edge.dest_node.lemmas) in connected_lemma_keys
                    )
                    if source_connected or dest_connected:
                        selected.add(idx)
        else:
            selected = set(valid_indices)
            for i in selected:
                edges[i - 1].forget_score = self.edge_forget
                edges[i - 1].active = True
                self._record_edge_in_graphs(edges[i - 1], self._current_sentence_index)

        # Preserve connectivity in the active projection.
        # If deactivating an edge would disconnect active nodes,
        # keep it active as a low-salience bridge (paper-consistent).
        def _active_subgraph_connected():
            G = nx.Graph()
            for e in edges:
                if e.active:
                    G.add_edge(e.source_node, e.dest_node)
            for n in active_nodes:
                G.add_node(n)
            return nx.is_connected(G) if G.number_of_nodes() > 1 else True

        # Non-salient memory edges (AMoC-consistent):
        # Keep edges in memory but mark them inactive.
        for j in range(1, len(edges) + 1):
            edge = edges[j - 1]
            if j not in selected and edges[j - 1] not in newly_added_edges:
                edge.active = False

                if not _active_subgraph_connected():
                    edge.active = True  # keep as bridge
                    edge.forget_score = 0  # lowest salience
                    # Record bridge edges in active_graph to prevent disconnection in plots
                    self._record_edge_in_graphs(edge, self._current_sentence_index)
                else:
                    self._record_edge_in_graphs(edge, self._current_sentence_index)

        # Final connectivity sweep - ensure the entire graph remains connected
        self._enforce_graph_connectivity()

    def init_graph(self, sent: Span) -> None:
        current_sentence_text_based_nodes, current_sentence_text_based_words = (
            self.get_senteces_text_based_nodes([sent], create_unexistent_nodes=True)
        )

        nodes_from_text = ""
        for i, node in enumerate(current_sentence_text_based_nodes):
            nodes_from_text += (
                f" - ({current_sentence_text_based_words[i]}, {node.node_type})\n"
            )

        relationships = self.client.get_new_relationships_first_sentence(
            nodes_from_text, sent.text, self.persona
        )
        # print(f"First sentence edges:\n{relationships}")

        for relationship in relationships:
            if len(relationship) != 3:
                continue
            if not relationship[0] or not relationship[2]:
                continue
            if relationship[0] == relationship[2]:
                continue
            if not isinstance(relationship[0], str) or not isinstance(
                relationship[2], str
            ):
                continue
            norm_subj = self._normalize_endpoint_text(relationship[0], is_subject=True)
            norm_obj = self._normalize_endpoint_text(relationship[2], is_subject=False)
            if norm_subj is None or norm_obj is None:
                continue
            if not self._passes_attachment_constraint(
                norm_subj,
                norm_obj,
                current_sentence_text_based_words,
                current_sentence_text_based_nodes,
                list(self.graph.nodes),
                self._get_nodes_with_active_edges(),
            ):
                continue
            source_node = self.get_node_from_text(
                norm_subj,
                current_sentence_text_based_nodes,
                current_sentence_text_based_words,
                node_source=NodeSource.TEXT_BASED,
                create_node=True,
            )
            dest_node = self.get_node_from_text(
                norm_obj,
                current_sentence_text_based_nodes,
                current_sentence_text_based_words,
                node_source=NodeSource.TEXT_BASED,
                create_node=True,
            )
            edge_label = relationship[1].replace("(edge)", "").strip()
            if not self._is_valid_relation_label(edge_label):
                continue
            if source_node is None or dest_node is None:
                continue
            self._add_edge(source_node, dest_node, edge_label, self.edge_forget)

    def add_inferred_relationships_to_graph_step_0(
        self,
        inferred_relationships: List[Tuple[str, str, str]],
        node_type: NodeType,
        sent: Span,
        explicit_nodes: Optional[List[Node]] = None,
        explicit_words: Optional[List[str]] = None,
    ) -> None:
        """Add inferred relationships for first sentence using hub-first radial attachment.

        Two-pass approach:
        1. First pass: Add relationships where at least one endpoint is explicit (hub-attached)
        2. Second pass: Add remaining relationships only if they connect to already-added inferred nodes
        """
        current_sentence_text_based_nodes, current_sentence_text_based_words = (
            self.get_senteces_text_based_nodes([sent], create_unexistent_nodes=False)
        )
        # Use provided explicit nodes or fall back to extracted ones
        filter_explicit_nodes = explicit_nodes or current_sentence_text_based_nodes
        filter_explicit_words = explicit_words or current_sentence_text_based_words

        # Track nodes added during inference for second-pass attachment
        inference_connected_nodes: set[Node] = set()
        explicit_lemma_keys = {tuple(n.lemmas) for n in filter_explicit_nodes}
        explicit_word_set = {w.lower() for w in filter_explicit_words}

        def _lemma_key(text: str) -> tuple[str, ...]:
            return tuple(get_concept_lemmas(self.spacy_nlp, text))

        def _is_explicit(text: str) -> bool:
            return (
                text.lower() in explicit_word_set
                or _lemma_key(text) in explicit_lemma_keys
            )

        # Separate relationships into hub-attached vs inferred-only
        hub_attached: List[Tuple[str, str, str]] = []
        inferred_only: List[Tuple[str, str, str]] = []

        for relationship in inferred_relationships:
            if len(relationship) != 3:
                continue
            if not relationship[0] or not relationship[2]:
                continue
            if relationship[0] == relationship[2]:
                continue
            if not isinstance(relationship[0], str) or not isinstance(
                relationship[2], str
            ):
                continue

            subj_is_explicit = _is_explicit(relationship[0])
            obj_is_explicit = _is_explicit(relationship[2])

            if subj_is_explicit or obj_is_explicit:
                hub_attached.append(relationship)
            else:
                inferred_only.append(relationship)

        def _process_relationship(relationship: Tuple[str, str, str]) -> Optional[Edge]:
            """Process a single relationship and return the edge if created."""
            norm_subj = self._normalize_endpoint_text(relationship[0], is_subject=True)
            norm_obj = self._normalize_endpoint_text(relationship[2], is_subject=False)
            if norm_subj is None or norm_obj is None:
                return None
            if not (
                self._appears_in_story(relationship[0])
                or self._appears_in_story(relationship[2])
            ):
                return None
            if not self._passes_attachment_constraint(
                relationship[0],
                relationship[2],
                current_sentence_text_based_words,
                current_sentence_text_based_nodes,
                list(self.graph.nodes),
                self._get_nodes_with_active_edges(),
            ):
                return None
            subj, subj_type = self._canonicalize_and_classify_node_text(relationship[0])
            obj, obj_type = self._canonicalize_and_classify_node_text(relationship[2])
            if subj_type is None or obj_type is None:
                return None
            source_node = self.get_node_from_text(
                norm_subj,
                current_sentence_text_based_nodes,
                current_sentence_text_based_words,
                node_source=NodeSource.INFERENCE_BASED,
                create_node=False,
            )
            dest_node = self.get_node_from_text(
                norm_obj,
                current_sentence_text_based_nodes,
                current_sentence_text_based_words,
                node_source=NodeSource.INFERENCE_BASED,
                create_node=False,
            )
            edge_label = relationship[1].replace("(edge)", "").strip()
            if not self._is_valid_relation_label(edge_label):
                return None
            if source_node is None:
                source_node = self.graph.add_or_get_node(
                    get_concept_lemmas(self.spacy_nlp, subj),
                    subj,
                    subj_type,
                    NodeSource.INFERENCE_BASED,
                )
            if dest_node is None:
                dest_node = self.graph.add_or_get_node(
                    get_concept_lemmas(self.spacy_nlp, obj),
                    obj,
                    obj_type,
                    NodeSource.INFERENCE_BASED,
                )
            return self._add_edge(source_node, dest_node, edge_label, self.edge_forget)

        # =====================================================
        # PASS 1: Hub-attached relationships (explicit backbone connected)
        # =====================================================
        logging.debug(
            "Step 0 Pass 1: Processing %d hub-attached inferences", len(hub_attached)
        )
        for relationship in hub_attached:
            edge = _process_relationship(relationship)
            if edge:
                inference_connected_nodes.add(edge.source_node)
                inference_connected_nodes.add(edge.dest_node)

        # =====================================================
        # PASS 2: Inferred-only relationships (must attach to Pass 1 nodes)
        # =====================================================
        if not self.strict_attachament_constraint:
            # In non-strict mode, process all inferred-only relationships
            logging.debug(
                "Step 0 Pass 2 (non-strict): Processing %d inferred-only relationships",
                len(inferred_only),
            )
            for relationship in inferred_only:
                _process_relationship(relationship)
        else:
            # In strict mode, only accept if one endpoint connects to inference_connected_nodes
            logging.debug(
                "Step 0 Pass 2 (strict): Processing %d inferred-only relationships",
                len(inferred_only),
            )
            inference_lemma_keys = {tuple(n.lemmas) for n in inference_connected_nodes}

            for relationship in inferred_only:
                subj_key = _lemma_key(relationship[0])
                obj_key = _lemma_key(relationship[2])
                attaches_to_inference = (
                    subj_key in inference_lemma_keys or obj_key in inference_lemma_keys
                )
                if not attaches_to_inference:
                    logging.debug(
                        "Rejected inferred-only (no attachment to backbone-connected inference): %s",
                        relationship,
                    )
                    continue
                edge = _process_relationship(relationship)
                if edge:
                    # Update connected nodes for potential further chaining
                    inference_connected_nodes.add(edge.source_node)
                    inference_connected_nodes.add(edge.dest_node)
                    inference_lemma_keys.add(tuple(edge.source_node.lemmas))
                    inference_lemma_keys.add(tuple(edge.dest_node.lemmas))

    def add_inferred_relationships_to_graph(
        self,
        inferred_relationships: List[Tuple[str, str, str]],
        node_type: NodeType,
        curr_sentences_nodes: List[Node],
        curr_sentences_words: List[str],
        active_graph_nodes: List[Node],
        added_edges: List[Edge],
    ) -> None:
        """Add inferred relationships using hub-first radial attachment.

        Two-pass approach:
        1. First pass: Add relationships where at least one endpoint is explicit (hub-attached)
        2. Second pass: Add remaining relationships only if they connect to already-added inferred nodes
        """
        # Track nodes added during inference for second-pass attachment
        inference_connected_nodes: set[Node] = set()

        # Pre-process relationships to validate and categorize
        explicit_lemma_keys = {tuple(n.lemmas) for n in curr_sentences_nodes}
        explicit_word_set = {w.lower() for w in curr_sentences_words}

        def _lemma_key(text: str) -> tuple[str, ...]:
            return tuple(get_concept_lemmas(self.spacy_nlp, text))

        def _is_explicit(text: str) -> bool:
            return (
                text.lower() in explicit_word_set
                or _lemma_key(text) in explicit_lemma_keys
            )

        # Separate relationships into hub-attached vs inferred-only
        hub_attached: List[Tuple[str, str, str]] = []
        inferred_only: List[Tuple[str, str, str]] = []

        for relationship in inferred_relationships:
            if len(relationship) != 3:
                continue
            if not relationship[0] or not relationship[2]:
                continue
            if relationship[0] == relationship[2]:
                continue
            if not isinstance(relationship[0], str) or not isinstance(
                relationship[2], str
            ):
                continue

            subj_is_explicit = _is_explicit(relationship[0])
            obj_is_explicit = _is_explicit(relationship[2])

            if subj_is_explicit or obj_is_explicit:
                hub_attached.append(relationship)
            else:
                inferred_only.append(relationship)

        def _process_relationship(relationship: Tuple[str, str, str]) -> Optional[Edge]:
            """Process a single relationship and return the edge if created."""
            norm_subj = self._normalize_endpoint_text(relationship[0], is_subject=True)
            norm_obj = self._normalize_endpoint_text(relationship[2], is_subject=False)
            if norm_subj is None or norm_obj is None:
                return None
            if not (
                self._appears_in_story(relationship[0])
                or self._appears_in_story(relationship[2])
            ):
                return None
            if not self._passes_attachment_constraint(
                relationship[0],
                relationship[2],
                curr_sentences_words,
                curr_sentences_nodes,
                active_graph_nodes,
                self._get_nodes_with_active_edges(),
            ):
                return None
            subj, subj_type = self._canonicalize_and_classify_node_text(relationship[0])
            obj, obj_type = self._canonicalize_and_classify_node_text(relationship[2])
            if subj_type is None or obj_type is None:
                return None
            source_node = self.get_node_from_new_relationship(
                norm_subj,
                active_graph_nodes,
                curr_sentences_nodes,
                curr_sentences_words,
                node_source=NodeSource.INFERENCE_BASED,
                create_node=False,
            )
            dest_node = self.get_node_from_new_relationship(
                norm_obj,
                active_graph_nodes,
                curr_sentences_nodes,
                curr_sentences_words,
                node_source=NodeSource.INFERENCE_BASED,
                create_node=False,
            )
            edge_label = relationship[1].replace("(edge)", "").strip()
            if not self._is_valid_relation_label(edge_label):
                return None
            if source_node is None:
                source_node = self.graph.add_or_get_node(
                    get_concept_lemmas(self.spacy_nlp, subj),
                    subj,
                    subj_type,
                    NodeSource.INFERENCE_BASED,
                )
            if dest_node is None:
                dest_node = self.graph.add_or_get_node(
                    get_concept_lemmas(self.spacy_nlp, obj),
                    obj,
                    obj_type,
                    NodeSource.INFERENCE_BASED,
                )
            return self._add_edge(source_node, dest_node, edge_label, self.edge_forget)

        # =====================================================
        # PASS 1: Hub-attached relationships (explicit backbone connected)
        # =====================================================
        logging.debug(
            "Pass 1: Processing %d hub-attached inferences", len(hub_attached)
        )
        for relationship in hub_attached:
            edge = _process_relationship(relationship)
            if edge:
                added_edges.append(edge)
                inference_connected_nodes.add(edge.source_node)
                inference_connected_nodes.add(edge.dest_node)

        # =====================================================
        # PASS 2: Inferred-only relationships (must attach to Pass 1 nodes)
        # =====================================================
        if not self.strict_attachament_constraint:
            # In non-strict mode, process all inferred-only relationships
            logging.debug(
                "Pass 2 (non-strict): Processing %d inferred-only relationships",
                len(inferred_only),
            )
            for relationship in inferred_only:
                edge = _process_relationship(relationship)
                if edge:
                    added_edges.append(edge)
        else:
            # In strict mode, only accept if one endpoint connects to inference_connected_nodes
            logging.debug(
                "Pass 2 (strict): Processing %d inferred-only relationships",
                len(inferred_only),
            )
            inference_lemma_keys = {tuple(n.lemmas) for n in inference_connected_nodes}

            for relationship in inferred_only:
                subj_key = _lemma_key(relationship[0])
                obj_key = _lemma_key(relationship[2])
                attaches_to_inference = (
                    subj_key in inference_lemma_keys or obj_key in inference_lemma_keys
                )
                if not attaches_to_inference:
                    logging.debug(
                        "Rejected inferred-only (no attachment to backbone-connected inference): %s",
                        relationship,
                    )
                    continue
                edge = _process_relationship(relationship)
                if edge:
                    added_edges.append(edge)
                    # Update connected nodes for potential further chaining
                    inference_connected_nodes.add(edge.source_node)
                    inference_connected_nodes.add(edge.dest_node)
                    inference_lemma_keys.add(tuple(edge.source_node.lemmas))
                    inference_lemma_keys.add(tuple(edge.dest_node.lemmas))

    def get_node_from_text(
        self,
        text: str,
        curr_sentences_nodes: List[Node],
        curr_sentences_words: List[str],
        node_source: NodeSource,
        create_node: bool,
    ) -> Optional[Node]:
        if text in curr_sentences_words:
            return curr_sentences_nodes[curr_sentences_words.index(text)]
        if create_node:
            canon, inferred_type = self._canonicalize_and_classify_node_text(text)
            if inferred_type is None:
                return None
            lemmas = get_concept_lemmas(self.spacy_nlp, canon)
            return self.graph.add_or_get_node(lemmas, canon, inferred_type, node_source)
        return None

    def get_node_from_new_relationship(
        self,
        text: str,
        graph_active_nodes: List[Node],
        curr_sentences_nodes: List[Node],
        curr_sentences_words: List[str],
        node_source: NodeSource,
        create_node: bool,
    ) -> Optional[Node]:
        if text in curr_sentences_words:
            return curr_sentences_nodes[curr_sentences_words.index(text)]
        else:
            canon, inferred_type = self._canonicalize_and_classify_node_text(text)
            if inferred_type is None:
                return None
            lemmas = get_concept_lemmas(self.spacy_nlp, canon)
            for node in graph_active_nodes:
                if lemmas == node.lemmas:
                    return node
        if create_node:
            canon, inferred_type = self._canonicalize_and_classify_node_text(text)
            if inferred_type is None:
                return None
            lemmas = get_concept_lemmas(self.spacy_nlp, canon)
            return self.graph.add_or_get_node(lemmas, canon, inferred_type, node_source)
        return None

    def is_content_word_and_non_stopword(
        self,
        token: Token,
        pos_list: List[str] = [
            "NOUN",
            "PROPN",
            "ADJ",
        ],
    ) -> bool:
        return (token.pos_ in pos_list) and (
            token.lemma_ not in self.spacy_nlp.Defaults.stop_words
        )

    def get_senteces_text_based_nodes(
        self, previous_sentences: List[Span], create_unexistent_nodes: bool = True
    ) -> Tuple[List[Node], List[str]]:
        text_based_nodes = []
        text_based_words = []
        for sent in previous_sentences:
            content_words = get_content_words_from_sent(self.spacy_nlp, sent)
            for word in content_words:
                node = self.graph.get_node([word.lemma_])
                if node is not None:
                    node.add_actual_text(word.text)
                    text_based_nodes.append(node)
                    text_based_words.append(word.text)
                else:
                    if create_unexistent_nodes:
                        if word.pos_ == "ADJ":
                            new_node = self.graph.add_or_get_node(
                                [word.lemma_],
                                word.text,
                                NodeType.PROPERTY,
                                NodeSource.TEXT_BASED,
                            )
                        else:
                            new_node = self.graph.add_or_get_node(
                                [word.lemma_],
                                word.text,
                                NodeType.CONCEPT,
                                NodeSource.TEXT_BASED,
                            )
                        text_based_nodes.append(new_node)
                        text_based_words.append(word.text)
        return text_based_nodes, text_based_words
